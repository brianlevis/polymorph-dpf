{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"codebase-pm-dpf\"\n",
    "\n",
    "objects = s3client.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "bucket = s3.Bucket(name=bucket_name)\n",
    "\n",
    "file_keys = []\n",
    "test_keys = []\n",
    "for n in range(20):\n",
    "    file_keys.append(\"13/15/part-00%03d.gz\" % n)\n",
    "    test_keys.append(\"14/15/part-00%03d.gz\" % n)\n",
    "    \n",
    "if os.path.exists(\"input.gz\"):\n",
    "    os.remove(\"input.gz\")\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/15/part-00000.gz\n",
      "13/15/part-00001.gz\n",
      "13/15/part-00002.gz\n",
      "13/15/part-00003.gz\n",
      "13/15/part-00004.gz\n",
      "13/15/part-00005.gz\n",
      "13/15/part-00006.gz\n",
      "13/15/part-00007.gz\n",
      "13/15/part-00008.gz\n",
      "13/15/part-00009.gz\n",
      "13/15/part-00010.gz\n",
      "13/15/part-00011.gz\n",
      "13/15/part-00012.gz\n",
      "13/15/part-00013.gz\n",
      "13/15/part-00014.gz\n",
      "13/15/part-00015.gz\n",
      "13/15/part-00016.gz\n",
      "13/15/part-00017.gz\n",
      "13/15/part-00018.gz\n",
      "13/15/part-00019.gz\n"
     ]
    }
   ],
   "source": [
    "for file_key in file_keys:\n",
    "    print(file_key)\n",
    "    bucket.download_file(Key=file_key, Filename=\"input.gz\")\n",
    "    \n",
    "    with gzip.open(\"input.gz\", 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        values = []\n",
    "        \n",
    "        values.append(data.get(\"ad_type\", [\"\"])[0])\n",
    "        values.append(data.get(\"geo_country_code2\", \"\"))\n",
    "        values.append(data.get(\"rate_metric\", \"\"))\n",
    "        values.append(str(data.get(\"site_id\", \"\")))\n",
    "        values.append(data.get(\"geo_timezone\", \"\"))\n",
    "        values.append(data.get(\"ua_device_type\", \"\"))\n",
    "        values.append(str(len(data.get(\"bid_requests\", []))))\n",
    "        values.append(data.get(\"ua_os\", \"\"))\n",
    "        values.append(str(data.get(\"zone_id\", \"\")))\n",
    "        values.append(data.get(\"geo_continent_code\", \"\"))\n",
    "        values.append(data.get(\"ua_os_name\", \"\"))\n",
    "        values.append(data.get(\"ua_device\", \"\"))\n",
    "        values.append(data.get(\"ua_name\", \"\"))\n",
    "        values.append(str(data.get(\"geo_area_code\", \"\")))\n",
    "        values.append(data.get(\"geo_city_name\", \"\"))\n",
    "        values.append(str(data.get(\"r_timestamp\", data.get(\"i_timestamp\", \"1T15\")).split(\"T\")[1][:2]))\n",
    "        \n",
    "        if len(data[\"bid_responses\"]) == 0:\n",
    "            continue\n",
    "        if len(data[\"bid_responses\"]) == 1:\n",
    "            label = data[\"bid_responses\"][0][\"bid_price\"] / 2\n",
    "        else:\n",
    "            bids = sorted([x[\"bid_price\"] for x in data[\"bid_responses\"]])\n",
    "            label = bids[-2] + (bids[-1] - bids[-2]) / 2\n",
    "        \n",
    "        train_data.append(values)\n",
    "        train_labels.append(label)\n",
    "    os.remove(\"input.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/15/part-00000.gz\n",
      "14/15/part-00001.gz\n",
      "14/15/part-00002.gz\n",
      "14/15/part-00003.gz\n",
      "14/15/part-00004.gz\n",
      "14/15/part-00005.gz\n",
      "14/15/part-00006.gz\n",
      "14/15/part-00007.gz\n",
      "14/15/part-00008.gz\n",
      "14/15/part-00009.gz\n",
      "14/15/part-00010.gz\n",
      "14/15/part-00011.gz\n",
      "14/15/part-00012.gz\n",
      "14/15/part-00013.gz\n",
      "14/15/part-00014.gz\n",
      "14/15/part-00015.gz\n",
      "14/15/part-00016.gz\n",
      "14/15/part-00017.gz\n",
      "14/15/part-00018.gz\n",
      "14/15/part-00019.gz\n"
     ]
    }
   ],
   "source": [
    "for file_key in test_keys:\n",
    "    print(file_key)\n",
    "    bucket.download_file(Key=file_key, Filename=\"input.gz\")\n",
    "    \n",
    "    with gzip.open(\"input.gz\", 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        values = []\n",
    "        \n",
    "        values.append(data.get(\"ad_type\", [\"\"])[0])\n",
    "        values.append(data.get(\"geo_country_code2\", \"\"))\n",
    "        values.append(data.get(\"rate_metric\", \"\"))\n",
    "        values.append(str(data.get(\"site_id\", \"\")))\n",
    "        values.append(data.get(\"geo_timezone\", \"\"))\n",
    "        values.append(data.get(\"ua_device_type\", \"\"))\n",
    "        values.append(str(len(data.get(\"bid_requests\", []))))\n",
    "        values.append(data.get(\"ua_os\", \"\"))\n",
    "        values.append(str(data.get(\"zone_id\", \"\")))\n",
    "        values.append(data.get(\"geo_continent_code\", \"\"))\n",
    "        values.append(data.get(\"ua_os_name\", \"\"))\n",
    "        values.append(data.get(\"ua_device\", \"\"))\n",
    "        values.append(data.get(\"ua_name\", \"\"))\n",
    "        values.append(str(data.get(\"geo_area_code\", \"\")))\n",
    "        values.append(data.get(\"geo_city_name\", \"\"))\n",
    "        values.append(str(data.get(\"r_timestamp\", data.get(\"i_timestamp\", \"1T15\")).split(\"T\")[1][:2]))\n",
    "        \n",
    "        if len(data[\"bid_responses\"]) == 0:\n",
    "            continue\n",
    "        if len(data[\"bid_responses\"]) == 1:\n",
    "            label = data[\"bid_responses\"][0][\"bid_price\"] / 2\n",
    "        else:\n",
    "            bids = sorted([x[\"bid_price\"] for x in data[\"bid_responses\"]])\n",
    "            label = bids[-2] + (bids[-1] - bids[-2]) / 2\n",
    "        \n",
    "        test_data.append(values)\n",
    "        test_labels.append(label)\n",
    "    os.remove(\"input.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396456\n",
      "1396456\n",
      "1337029\n",
      "1337029\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0d7f5b7278>, '_num_worker_replicas': 0, '_tf_random_seed': None, '_model_dir': './', '_save_summary_steps': 100, '_task_id': 0, '_log_step_count_steps': 100, '_task_type': None, '_session_config': None, '_environment': 'local', '_is_chief': True, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_master': '', '_save_checkpoints_secs': 600}\n",
      "WARNING:tensorflow:From <ipython-input-5-c0a664641024>:13: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-5-c0a664641024>:13: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Constructing forest with params = \n",
      "INFO:tensorflow:{'base_random_seed': 0, 'leaf_model_type': 2, 'param_file': None, 'prune_every_samples': 0, 'num_classes': 1, 'num_output_columns': 2, 'early_finish_check_every_samples': 0, 'collate_examples': False, 'dominate_method': 'bootstrap', 'inference_tree_paths': False, 'num_trees': 100, 'bagged_num_features': 16, 'num_splits_to_consider': 10, 'model_name': 'all_dense', 'num_features': 16, 'dominate_fraction': 0.99, 'initialize_average_splits': False, 'feature_bagging_fraction': 1.0, 'split_after_samples': 250, 'max_nodes': 10000, 'split_pruning_name': 'none', 'split_name': 'less_or_equal', 'stats_model_type': 2, 'bagging_fraction': 1.0, 'regression': True, 'checkpoint_stats': False, 'finish_type': 0, 'split_type': 0, 'use_running_stats_method': False, 'valid_leaf_threshold': 1, 'num_outputs': 1, 'bagged_features': None, 'split_finish_name': 'basic', 'max_fertile_nodes': 0, 'pruning_type': 0}\n",
      "INFO:tensorflow:dense_features_size: 16 dense: [{name: features original_type: 1 size: 16}] sparse: []\n",
      "INFO:tensorflow:dense_features_size: 16 dense: [{name: features original_type: 1 size: 16}] sparse: []\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 3.8575497e-05, step = 1\n",
      "INFO:tensorflow:TensorForestLossHook resetting last_step.\n",
      "INFO:tensorflow:Saving checkpoints for 16 into ./model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 25 into ./model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 31 into ./model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'_Resource' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))\n",
    "params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n",
    "    num_classes=1, # num dimensions of output\n",
    "    num_features=len(train_data[0]), # should be 16\n",
    "    regression=True\n",
    ")\n",
    "\n",
    "classifier = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params, model_dir=\"./\")\n",
    "\n",
    "classifier.fit(x=np.array(train_data), y=np.array(train_labels))\n",
    "\n",
    "y_out = classifier.predict(x=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
